{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9be72b-4436-42a8-8310-8b80099384ef",
   "metadata": {},
   "source": [
    "# Dead-Simple Boost Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d81442-78e5-42bf-baf0-f9849b246b35",
   "metadata": {},
   "source": [
    "This notebook was prepared for the Kaggle October 2021 tabular-data competition, with the objective of creating a high performing model with minimal manual effort. Therefore, we're going with an ensemble of boosting models. As the tree construction used by XGBoost and LightGBM, we'll want to use both of those models in our ensemble, as well as the potential addition of a tabular-MLP model if time permits. The competition data is pretty simple, it's a binary classification problem with a mixture of normalized float & boolean features. The data is balanced & scored via ROC auc. \n",
    "\n",
    "Behind the scenes, this script does some pretty cool stuff --at least I think so\n",
    "\n",
    "1) Converts source data to a HDF5 table. The hdf5 data is grouped like '/{train|test}{0-N}{data|target}', The {0-N} seems a little wonky, but it allows the data to be further separated into minibatches, which makes multiprocessing super easy. Combining this with pytorch's Dataloader cuts the time to load our data into memory from 60s (using the original csv file) to 3s. The {train|test} split is automatically stratified (even though it's not necessary for this dataset).\n",
    "\n",
    "2) The ModelBuilder has a hook for our MLFlow server. This makes keeping track of experiments much easier. After running this script for a while, we can run 'get_best_models()' which will return the N best performing models. This is especially useful later on when we want to build out our ensemble.\n",
    "\n",
    "3) ModelBuilder also incorporates hyperopt. You'll still need to define the models' valid hyperparameter search space, but we now get to use TPE when determining our next search params, a massive improvement over grid-search or random search.\n",
    "\n",
    "4) The models (XGBModel or LGBModel) are trained using the GPU. We use kfold cross-validation on our training data (split in to train/val) to compute the run's average auc metric. Once finished the model is retrained using the entire training data & uploaded to the MLFlow artifact store.\n",
    "\n",
    "5) Finally, we can query the MLFlow server to get the top N XGBModels and top N LGBModels. Assign weights to them and plug them into our ensembler. (Not coded out yet, but the logic is pretty simple).\n",
    "\n",
    "6) Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6813f-5021-4ec3-9906-cf76f42ea35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdf_utils import HDFDatasetBuilder,HDFLoader\n",
    "from modelbuilder import ModelBuilder\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5259c4-f673-45ae-87fd-6258bf322db5",
   "metadata": {},
   "source": [
    "### Transform source data into a more computationally friendly datastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476c066-d3f1-49f6-99a1-14bfa6c5838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFDatasetBuilder(\n",
    "    source_filepath='data/train.csv',\n",
    "    output_filepath='data.hdf5',\n",
    "    desired_batchsize = 50000,\n",
    "    expected_workers = 10,\n",
    "    test_sample_size = 0.15,\n",
    "    shuffle_samples = True,\n",
    "    stratify_data = True,\n",
    "    features_to_exclude = ['id'],\n",
    "    target_column_name = 'target'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1523ebf-323b-4fae-8b11-0bb647c079de",
   "metadata": {},
   "source": [
    "### Define hyperparameter search spaces for LightGBM and XGBoost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd2c5e-12e0-403f-b5b7-1bbf66e0027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_hyperopt_space = {\n",
    "    'reg_alpha': hp.uniform('reg_alpha',0.0,0.3),\n",
    "    'reg_lambda': hp.uniform('reg_lambda',0.0,0.2),\n",
    "    'learning_rate': hp.uniform('learning_rate',0.01,0.25),\n",
    "    'num_leaves': hp.choice('num_leaves', range(20,50,1)),\n",
    "    'max_depth': hp.choice('max_depth', range(2,12,1)),\n",
    "    'n_estimators': hp.choice('n_estimators',range(1000,10000,1)),\n",
    "    'subsample': hp.uniform('subsample',0.5,1.0),\n",
    "}\n",
    "\n",
    "xgb_hyperopt_space = {\n",
    "    'eta': hp.uniform('eta', 0.01, 0.15),\n",
    "    'max_depth': hp.choice('max_depth', range(2, 8, 1)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 0.75),\n",
    "    'n_estimators': hp.choice('n_estimators',range(100,101,1))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76caf99e-4424-40c4-a92e-92d3b86e2907",
   "metadata": {},
   "source": [
    "### Run Hyperparameter search to find optimal model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52674ade-dbda-46ab-8e7d-7eea92173b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder()\n",
    "while True:\n",
    "    model_builder.run(\n",
    "        experiment_name='taboct-lgbm',\n",
    "        model_type='LGBModel',\n",
    "        hyperopt_space=lgb_hyperopt_space,\n",
    "    )\n",
    "    \n",
    "    model_builder.run(\n",
    "        experiment_name='taboct-xgbm',\n",
    "        model_type='XGBModel',\n",
    "        hyperopt_space=xgb_hyperopt_space,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedbbca-8d81-4d0d-8e4c-b4d8aaaed292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
