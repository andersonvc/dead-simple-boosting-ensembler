{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524220a7-0bd4-4114-9858-ee8ae207dbe3",
   "metadata": {},
   "source": [
    "# Pre-process Source Data & Create Derived Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e354b9-ec23-41b3-bf67-47c588c3b3cc",
   "metadata": {},
   "source": [
    "This notebook is a lot messier than boost-ensemble-notebook. In this notebook, we'll clip and normalize all our feature. (any feature 2std away from the mean are clipped). Next, we'll identify the top N features most correlated with the target value. These features will be used to fit N kmeans clusters. Each sample will include a new derived feature for the samples' Euclidean distance to each of the generated clusters.\n",
    "https://www.kaggle.com/motchan/tps-oct-2021-kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30315f5d-dbc1-452b-ad83-d983dfd7ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "from hdf_utils import HDFBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd67ea-5ba2-4c84-9ae0-d5b876712a94",
   "metadata": {},
   "source": [
    "#### Load data into memory and convert to np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f5ca8dc-2ae1-4624-8d39-040fffc77d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv',index_col=None)\n",
    "test_data = pd.read_csv('data/test.csv',index_col=None)\n",
    "\n",
    "train_data.pop('id')\n",
    "test_id = test_data.pop('id')\n",
    "\n",
    "column_names = train_data.columns.tolist()\n",
    "y_train = train_data.pop('target').values.reshape(-1,1)\n",
    "\n",
    "X_train = train_data.values\n",
    "X_test = test_data.values\n",
    "\n",
    "del train_data,test_data\n",
    "out = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c33025-3aed-4921-b155-786176243271",
   "metadata": {},
   "source": [
    "#### Clip outliers and normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76489fa3-ea9b-4914-953e-a29985faf97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_in,sigma=2,scaler_filepath='preproc_scaler1.save'):\n",
    "\n",
    "    if os.path.exists(scaler_filepath):\n",
    "        scaler = joblib.load(scaler_filepath)\n",
    "        return scaler.transform(X_in)\n",
    "    \n",
    "    data_mean = X_in.mean(axis=0)\n",
    "    data_std = X_in.std(axis=0)\n",
    "    lower_bound = data_mean-sigma*data_std\n",
    "    upper_bound = data_mean+sigma*data_std\n",
    "\n",
    "    X_in = X_in.clip(lower_bound,upper_bound)\n",
    "    \n",
    "    from sklearn import preprocessing\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    scaler.fit(X_in)\n",
    "            \n",
    "    scaler_filepath = \"preproc_scaler1.save\"\n",
    "    joblib.dump(scaler,scaler_filepath)\n",
    "    \n",
    "    return scaler.transform(X_in)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa59105a-686a-4e53-ba48-e9bdc2d81171",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_split = X_train.shape[0]\n",
    "\n",
    "X_combined = np.concatenate([X_train,X_test])\n",
    "X_combined = preprocess(X_combined,sigma=2)\n",
    "X_train,X_test = X_combined[:test_train_split,:], X_combined[test_train_split:,:]\n",
    "\n",
    "del X_combined\n",
    "out = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e39cfde-0890-4488-a72a-7f5327203642",
   "metadata": {},
   "source": [
    "### Build new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0472780a-e076-4240-9e15-be0821b66675",
   "metadata": {},
   "source": [
    "#### Identify top N features most correlated to target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1070fe7-30b9-4e2e-ab72-c45fc9473024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_feature_indices(X,y,thresh=0.8):\n",
    "\n",
    "    corr_coef = abs(np.corrcoef(X_train,y_train,rowvar=False)[:-1,-1])\n",
    "    sorted_indices = np.argsort(corr_coef)[::-1]\n",
    "    feature_cnt = sorted_indices.shape[0]\n",
    "    \n",
    "    if isinstance(thresh,int):\n",
    "        return sorted_indices[:min(feature_cnt,thresh)]\n",
    "    \n",
    "    top_indices = []\n",
    "    curr_ix = 0\n",
    "    score = 0\n",
    "    \n",
    "    thresh = min(thresh,1.0)\n",
    "    weight_total = sum(corr_coef)\n",
    "    \n",
    "    while score/weight_total<thresh:\n",
    "        target_ix = sorted_indices[curr_ix]\n",
    "        score+=corr_coef[target_ix]\n",
    "        top_indices.append(target_ix)\n",
    "        curr_ix+=1\n",
    "        \n",
    "    return np.array(top_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49f8a9bf-f5de-4bc3-b72b-bced4a54a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = get_top_feature_indices(X_train,y_train,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf859a-0871-4191-a8d1-e973cee443d8",
   "metadata": {},
   "source": [
    "#### Determine Ideal Number of Clusters from Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a75d587-4774-43a5-a66d-9ba169fe00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters(X_in,selected_features,max_cluster_cnt=15):\n",
    "    silhouette_scores = []\n",
    "    for i in range(2,max_cluster_cnt):\n",
    "        kmeans_model = KMeans(\n",
    "            n_clusters=i, \n",
    "            init='k-means++',\n",
    "            max_iter=500,\n",
    "            random_state=1\n",
    "        ).fit(X_in[:,selected_features])\n",
    "        labels = kmeans_model.labels_\n",
    "        silhouette = metrics.silhouette_score(\n",
    "            X_in, \n",
    "            labels, \n",
    "            metric='euclidean',\n",
    "            sample_size=50000,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        silhouette_scores.append((i,silhouette))\n",
    "        print(f\"cluster_cnt: {i}, silhouette_score: {silhouette}\")\n",
    "    return silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7079e440-ebd8-47b7-b3ea-cde573e590b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_clusters = False\n",
    "\n",
    "if compute_clusters:\n",
    "    X_combined = np.concatenate([X_train,X_test])\n",
    "    analyze_clusters(X_combined,top_features,max_cluster_cnt=15)\n",
    "    del X_combined\n",
    "    out = gc.collect()\n",
    "\n",
    "desired_cluster_cnt = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c33f09-7d57-45f0-9c27-4ec0555d53f3",
   "metadata": {},
   "source": [
    "### Split train data into train & validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0a368e-b80c-48f6-a07b-2854dec109f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(\n",
    "    np.concatenate([X_train,y_train],axis=1),\n",
    "    columns=column_names\n",
    ")\n",
    "\n",
    "# Create train-test split\n",
    "train_data,val_data = model_selection.train_test_split(\n",
    "    train_data,\n",
    "    test_size=0.15,\n",
    "    shuffle=True,\n",
    "    stratify=train_data['target'],\n",
    ")\n",
    "\n",
    "train_data.reset_index(inplace=True,drop=True)\n",
    "val_data.reset_index(inplace=True,drop=True)\n",
    "test_data = pd.DataFrame(X_test,columns=column_names[:-1])\n",
    "\n",
    "train_target = train_data.pop('target')\n",
    "val_target = val_data.pop('target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dca8fb-4661-4101-87c6-6e88516a96be",
   "metadata": {},
   "source": [
    "#### Use KMeans Clustering to generate Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab4c9c4-ce02-4813-a988-7f5be7fd85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(\n",
    "    n_clusters=desired_cluster_cnt, \n",
    "    init=\"k-means++\", \n",
    "    max_iter=500, \n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "cluster_names = [f'cluster{i}' for i in range(desired_cluster_cnt)]\n",
    "\n",
    "train_cluster_cols = kmeans_model.fit_transform(train_data.iloc[:,top_features])\n",
    "train_cluster_cols = pd.DataFrame(train_cluster_cols,columns=cluster_names)\n",
    "\n",
    "val_cluster_cols = kmeans_model.transform(val_data.iloc[:,top_features])\n",
    "kmeans_model.fit(pd.concat([train_data,val_data],axis=0).iloc[:,top_features])\n",
    "val_cluster_cols = pd.DataFrame(val_cluster_cols,columns=cluster_names)\n",
    "\n",
    "train_data = pd.concat([train_data,train_cluster_cols],axis=1)\n",
    "val_data = pd.concat([val_data,val_cluster_cols],axis=1)\n",
    "del train_cluster_cols, val_cluster_cols\n",
    "\n",
    "test_cluster_cols = kmeans_model.transform(test_data.iloc[:,top_features])\n",
    "test_cluster_cols = pd.DataFrame(test_cluster_cols,columns=cluster_names)\n",
    "test_data = pd.concat([test_data,test_cluster_cols],axis=1)\n",
    "del test_cluster_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a67e55-28d7-4eb6-8007-2f00ab7cedd9",
   "metadata": {},
   "source": [
    "#### Create HDF5 record for processed train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2074ee20-1d49-4067-8ec4-72064e9a74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFBuilder().from_df(\n",
    "    X_train=train_data,\n",
    "    X_val=val_data,\n",
    "    y_train=train_target,\n",
    "    y_val=val_target,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77b345-bc48-4dff-baf9-232f429c96c4",
   "metadata": {},
   "source": [
    "#### Create CSV for processed test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f638a2-5f56-4c29-89fc-fb955092aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([test_id,test_data],axis=1).to_csv('data/processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602f92d-7f18-4f92-ad20-f1ae8885ac0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc746c96-11fa-4f86-8791-6be5ecf47b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
